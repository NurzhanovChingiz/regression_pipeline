{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "940c6448",
   "metadata": {},
   "source": [
    "# CFG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "081569e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.721066Z",
     "start_time": "2024-01-12T19:02:53.712683Z"
    }
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \"\"\"\n",
    "    Configuration class holding constants and configurations for data preprocessing.\n",
    "\n",
    "    Attributes:\n",
    "        PATH (str): Path to the dataset file.\n",
    "        TEST_SPLIT (float): Ratio for splitting the dataset into training and testing sets.\n",
    "        TARGET (str): The name of the target variable in the dataset.\n",
    "        SELECTED_COLUMNS (list of str): List of column names to be selected for processing.\n",
    "    \"\"\"\n",
    "    SAVE_PATH = 'ver1' # path to save\n",
    "    LOAD_PATH = 'ver1' # load from path  \n",
    "    SAVE_METHOD =  'pickle'\n",
    "    SEED = 42 \n",
    "    PATH = 'csv/data.csv'\n",
    "    TEST_SIZE = 0.2\n",
    "    N_JOBS = -1\n",
    "    CROSS_VALID = True\n",
    "    N_SPLIT = 2\n",
    "    BAGGING = True\n",
    "    BAGGING_ESTIMATORS = 30\n",
    "    TARGET = 'стоимость_за_кв_м_ob'\n",
    "    SELECTED_COLUMNS = [\n",
    "        'Кадастровый_номер_ob',\n",
    "        'Общая_площадь_м2_ob', 'Жилая_площадь_м2_ob',\n",
    "        'Этаж_ob',\n",
    "        'Этажность_ob', 'Количество_комнат_ob', 'Год_постройки_ob',\n",
    "        'Материал_стен_ob', 'Район_ob', 'Микрорайон_ob', 'Улица_ob', 'Дом_ob',\n",
    "        'Квартира_ob', 'стоимость_за_кв_м_ob', 'дата_ob', \"flat_1_cnt\",\n",
    "        \"flat_2_cnt\", \"flat_3_cnt\", \"flat_4_cnt\",\n",
    "        \"total_year_date_construction\", \"total_material_type\",\n",
    "        \"total_heat_type\", \"total_has_electric\", \"total_floor_count\",\n",
    "        \"total_hot_water_system\", \"total_heat_system\", \"total_district\",\n",
    "        \"total_lift\", 'to_metre', 'to_centre'\n",
    "    ]\n",
    "    \n",
    "    int_columns = [\n",
    "        'Этажность_ob',\n",
    "        'Количество_комнат_ob',\n",
    "        'Год_постройки_ob',\n",
    "#         'Этаж_ob',\n",
    "#         'Квартира_ob',  \n",
    "    ]\n",
    "    float_columns = [\n",
    "        'total_year_date_construction',\n",
    "        'стоимость_за_кв_м_ob',\n",
    "        'Общая_площадь_м2_ob',\n",
    "        'Жилая_площадь_м2_ob',\n",
    "        'flat_1_cnt',\n",
    "        'flat_2_cnt',\n",
    "        'flat_3_cnt',\n",
    "        'flat_4_cnt',\n",
    "    #     'abaya',\n",
    "        'to_metre',\n",
    "        'to_centre',\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a738ee34",
   "metadata": {},
   "source": [
    "# Set SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "c600934b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.725828Z",
     "start_time": "2024-01-12T19:02:53.722725Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seeding done!!!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "# Preven randomnes\n",
    "def seeding(SEED):\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "    print('seeding done!!!')\n",
    "    \n",
    "seeding(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d9c42a",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2499ed84",
   "metadata": {},
   "source": [
    "## Preprocessing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "328b4e61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.728777Z",
     "start_time": "2024-01-12T19:02:53.726519Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c44e73",
   "metadata": {},
   "source": [
    "# Mapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d678e",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "a2fca9bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.733559Z",
     "start_time": "2024-01-12T19:02:53.729649Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from category_encoders import CatBoostEncoder\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class GenerateFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # Nothing to do here\n",
    "\n",
    "    def transform(self, X):\n",
    "        df = X.copy()\n",
    "        \n",
    "        new_features = pd.DataFrame(index=df.index)\n",
    "        numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "        \n",
    "        for col in numerical_cols:\n",
    "    #         new_features[f\"{col}_global_mean\"] = [df[col].mean()]* len(df)\n",
    "    #         new_features[f\"{col}_global_std\"] = [df[col].std()]* len(df)\n",
    "    #         new_features[f\"{col}_global_median\"] = [df[col].median()]* len(df)\n",
    "    #         new_features[f\"{col}_global_diff\"] = [df[col].diff()]* len(df)\n",
    "            # Basic statistics\n",
    "            new_features[f'{col}_min'] = [df[col].min()] * len(df)\n",
    "            new_features[f'{col}_max'] = [df[col].max()] * len(df)\n",
    "            new_features[f'{col}_range'] = [df[col].max() - df[col].min()] * len(df)\n",
    "            new_features[f'{col}_sum'] = [df[col].sum()] * len(df)\n",
    "            new_features[f'{col}_var'] = [df[col].var()] * len(df)\n",
    "            new_features[f'{col}_coef_var'] = [df[col].std() / df[col].mean() if df[col].mean() != 0 else 0] * len(df)\n",
    "            new_features[f'{col}_skew'] = [df[col].skew()] * len(df)\n",
    "            new_features[f'{col}_kurt'] = [df[col].kurt()] * len(df)\n",
    "    \n",
    "            # Quantiles\n",
    "            new_features[f'{col}_25%'] = [df[col].quantile(0.25)] * len(df)\n",
    "            new_features[f'{col}_50%'] = [df[col].quantile(0.5)] * len(df)\n",
    "            new_features[f'{col}_75%'] = [df[col].quantile(0.75)] * len(df)\n",
    "    \n",
    "            # Cumulative statistics\n",
    "            new_features[f'{col}_cumsum'] = df[col].cumsum()\n",
    "            new_features[f'{col}_cummax'] = df[col].cummax()\n",
    "            new_features[f'{col}_cummin'] = df[col].cummin()\n",
    "    \n",
    "            # Exponential Moving Average\n",
    "            new_features[f'{col}_ema'] = df[col].ewm(span=10, adjust=False).mean()\n",
    "    \n",
    "        # Concatenate the new features with the original DataFrame\n",
    "        df = pd.concat([df, new_features], axis=1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "467dfa46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.737098Z",
     "start_time": "2024-01-12T19:02:53.734270Z"
    }
   },
   "outputs": [],
   "source": [
    "from feature_engine.selection import DropHighPSIFeatures, DropDuplicateFeatures, DropCorrelatedFeatures, DropConstantFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b70e5f48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.740157Z",
     "start_time": "2024-01-12T19:02:53.737663Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from category_encoders import CatBoostEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1be179",
   "metadata": {},
   "source": [
    "# CustomTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b301f9e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.746122Z",
     "start_time": "2024-01-12T19:02:53.740788Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.cont_cols = None\n",
    "        self.target = None\n",
    "        self.best_transformations = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \n",
    "        X_copy = X.copy()\n",
    "        self.cont_cols = X_copy.columns\n",
    "        self.target = y\n",
    "        self.best_transformations = {}\n",
    "        \n",
    "        for col in self.cont_cols:\n",
    "            best_auc = -1\n",
    "            best_transformation = None\n",
    "\n",
    "            # Log Transformation\n",
    "            X_copy['log_' + col] = np.log1p(X_copy[col])\n",
    "            auc_log = self._evaluate_transformation(X_copy[['log_' + col]], y)\n",
    "            if auc_log > best_auc:\n",
    "                best_auc = auc_log\n",
    "                best_transformation = 'log'\n",
    "\n",
    "            # Square Root Transformation\n",
    "            X_copy['sqrt_' + col] = np.sqrt(X_copy[col])\n",
    "            auc_sqrt = self._evaluate_transformation(X_copy[['sqrt_' + col]], y)\n",
    "            if auc_sqrt > best_auc:\n",
    "                best_auc = auc_sqrt\n",
    "                best_transformation = 'sqrt'\n",
    "\n",
    "            # Box-Cox Transformation\n",
    "            X_copy['boxcox_' + col] = PowerTransformer(method='box-cox').fit_transform(abs(X_copy[[col]])+ 1e-5)\n",
    "            auc_boxcox = self._evaluate_transformation(X_copy[['boxcox_' + col]], y)\n",
    "            if auc_boxcox > best_auc:\n",
    "                best_auc = auc_boxcox\n",
    "                best_transformation = 'boxcox'\n",
    "\n",
    "            self.best_transformations[col] = best_transformation\n",
    "#             X[[col]] = X[self.best_transformations[col]]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = X.copy()\n",
    "        for col in self.cont_cols:\n",
    "            if self.best_transformations[col] == 'log':\n",
    "                X_transformed[col] = np.log1p(X_transformed[col])\n",
    "            elif self.best_transformations[col] == 'sqrt':\n",
    "                X_transformed[col] = np.sqrt(X_transformed[col])\n",
    "            elif self.best_transformations[col] == 'boxcox':\n",
    "                X_transformed[col] = PowerTransformer(method='box-cox').fit_transform(abs(X_transformed[[col]]) + 1e-5)\n",
    "        mapper_pipe = self.mapper()\n",
    "        X_transformed = mapper_pipe.fit_transform(X_transformed, y)\n",
    "        return X_transformed\n",
    "\n",
    "    def _evaluate_transformation(self, X, y):\n",
    "        \n",
    "        model = HistGradientBoostingRegressor(random_state=42, warm_start=True, max_iter=20)\n",
    "        cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        auc_scores = cross_val_score(model, X, y, cv=cv, scoring=make_scorer(mean_absolute_percentage_error))\n",
    "#         print(auc_scores)\n",
    "\n",
    "        return np.mean(auc_scores)\n",
    "    def get_feature_names_out(self):\n",
    "        pass\n",
    "    def mapper(self):\n",
    "        self.cat_imputer =  SimpleImputer(strategy='most_frequent')\n",
    "        # Scale and encoding\n",
    "        self.scaler = RobustScaler(quantile_range=(10.0, 90.0))\n",
    "        self.encoder =  CatBoostEncoder(random_state = 42, drop_invariant=True)\n",
    "        num_cols = make_column_selector(dtype_include=np.number)\n",
    "        cat_cols = make_column_selector(dtype_include=object)\n",
    "        categorical_imputer = Pipeline([\n",
    "            ('Imputer', self.cat_imputer),\n",
    "            ('Encoder', self.encoder)  # Adding encoding for categorical data\n",
    "        ])\n",
    "        imput = ColumnTransformer([\n",
    "            ('categorical_imputer', categorical_imputer, cat_cols),\n",
    "            \n",
    "        ],\n",
    "            remainder='passthrough' # remainder='passthrough' to keep columns not specified\n",
    "        )  \n",
    "    \n",
    "        pipe = Pipeline([\n",
    "            \n",
    "            ('Imputer', imput),\n",
    "            \n",
    "            ('DropDuplicateFeatures', DropDuplicateFeatures()),\n",
    "            ('DropConstantFeatures', DropConstantFeatures()),\n",
    "            ('DropCorrelatedFeatures', DropCorrelatedFeatures(threshold=0.95)),\n",
    "            ('scaler', self.scaler),  # Applies scaling to numerical features only\n",
    "        ])\n",
    "\n",
    "        pipe.set_output(transform=\"pandas\")\n",
    "        return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9e728be5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.749551Z",
     "start_time": "2024-01-12T19:02:53.746797Z"
    }
   },
   "outputs": [],
   "source": [
    "# prep = Preprocessing(CFG)\n",
    "# X, y = prep.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0c2d3855",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.761333Z",
     "start_time": "2024-01-12T19:02:53.750175Z"
    }
   },
   "outputs": [],
   "source": [
    "# mapper = Mapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "dea91401",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.766458Z",
     "start_time": "2024-01-12T19:02:53.762641Z"
    }
   },
   "outputs": [],
   "source": [
    "# mapper = mapper(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "31a3d178",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.771625Z",
     "start_time": "2024-01-12T19:02:53.767469Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = mapper.fit_transform(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c0015f07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.777355Z",
     "start_time": "2024-01-12T19:02:53.773254Z"
    }
   },
   "outputs": [],
   "source": [
    "# trans = CustomTransformer()\n",
    "# trans.fit_transform(df,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c09d921b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.780504Z",
     "start_time": "2024-01-12T19:02:53.778646Z"
    }
   },
   "outputs": [],
   "source": [
    "# trans.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1dd19a74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.783709Z",
     "start_time": "2024-01-12T19:02:53.781703Z"
    }
   },
   "outputs": [],
   "source": [
    "# trans.best_transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df63c668",
   "metadata": {},
   "source": [
    "# IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "87ae8ce3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.790795Z",
     "start_time": "2024-01-12T19:02:53.784419Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "class IterativeImputer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, max_iterations=3, model_params={}):\n",
    "        self.max_iterations = max_iterations\n",
    "        self.model_params = model_params\n",
    "        self.features = None\n",
    "        self.mapper_ = self.mapper()\n",
    "        self.rows_miss = None\n",
    "        self.error_minimize = None\n",
    "        self.lgb_params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 5,\n",
    "            \"num_leaves\": 16,\n",
    "            'learning_rate': 0.05,\n",
    "            'subsample': 0.7,\n",
    "            'colsample_bytree': 0.8,\n",
    "            #'reg_alpha': 0.25,\n",
    "            'reg_lambda': 5e-07,\n",
    "            'objective': 'regression_l2',\n",
    "            'metric': 'mean_squared_error',\n",
    "#             'boosting_type': 'gbdt',\n",
    "#             'random_state': 42,\n",
    "        }\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the imputer to the data and identify features with missing values.\"\"\"\n",
    "        self.features = [f for f in X.columns if X[f].isna().sum() > 0]\n",
    "#         print(self.features)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_temp=X.copy()\n",
    "        \"\"\"Impute missing values using iterative imputation.\"\"\"\n",
    "        \n",
    "        missing_rows = self.store_missing_rows(X_temp, self.features)\n",
    "        for f in self.features:\n",
    "            X_temp[f]=X_temp[f].fillna(X_temp[f].mean())\n",
    "        cat_features = [f for f in X_temp.columns if not pd.api.types.is_numeric_dtype(X_temp[f])]\n",
    "        dictionary = {feature: [] for feature in self.features}\n",
    "        if len(self.features)>0:\n",
    "            for iteration in tqdm(range(self.max_iterations), desc=\"Iterations\"):\n",
    "    #             print(1)\n",
    "                for feature in self.features:\n",
    "            #                 # Skip features with no missing values\n",
    "                    self.rows_miss =  missing_rows[feature].index\n",
    "                    missing_temp = X_temp.loc[self.rows_miss].copy()\n",
    "                    non_missing_temp = X_temp.drop(index=self.rows_miss).copy()\n",
    "                    y_pred_prev=missing_temp[feature]\n",
    "                    missing_temp = missing_temp.drop(columns=[feature])\n",
    "                    # Step 3: Use the remaining features to predict missing values using Random Forests\n",
    "                    X_train = non_missing_temp.drop(columns=[feature])\n",
    "                    y_train = non_missing_temp[[feature]]\n",
    "                    mapper_pipe = self.mapper_\n",
    "                    \n",
    "                    X_train = mapper_pipe.fit_transform(X_train, y_train)\n",
    "                    model= lgb.LGBMRegressor(**self.lgb_params,random_state=42,boosting_type='dart')\n",
    "                    model.fit(X_train, y_train)\n",
    "                    # Step 4: Predict missing values for the feature and update all N features\n",
    "                    y_pred = model.predict(mapper_pipe.transform(missing_temp))\n",
    "                    X_temp.loc[self.rows_miss, feature] = y_pred\n",
    "                    self.error_minimize=self.rmse(y_pred,y_pred_prev)\n",
    "                    dictionary[feature].append(self.error_minimize)  # Append the error_minimize value\n",
    "#                     print(self.error_minimize)\n",
    "    #                 print(2)\n",
    "            X_temp[self.features] = np.array(X_temp.iloc[:X_temp.shape[0]][self.features])\n",
    "            X_temp = X_temp.drop(columns=cat_features)    \n",
    "            return X_temp\n",
    "        return X\n",
    "    def store_missing_rows(self, df, features):\n",
    "        \"\"\"Function stores where missing values are located for given set of features.\"\"\"\n",
    "        missing_rows = {}\n",
    "\n",
    "        for feature in features:\n",
    "            missing_rows[feature] = df[df[feature].isnull()]\n",
    "        \n",
    "        return missing_rows\n",
    "    def get_feature_names_out(self):\n",
    "        pass\n",
    "    def rmse(self, y1, y2):\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        \"\"\"RMSE Evaluator\"\"\"\n",
    "        return (np.sqrt(mean_squared_error(np.array(y1), np.array(y2))))\n",
    "\n",
    "        \n",
    "    def mapper(self):\n",
    "        self.cat_imputer =  SimpleImputer(strategy='most_frequent')\n",
    "        # Scale and encoding\n",
    "        self.scaler = RobustScaler(quantile_range=(10.0, 90.0))\n",
    "        self.encoder =  CatBoostEncoder(random_state = 42, drop_invariant=True)\n",
    "        num_cols = make_column_selector(dtype_include=np.number)\n",
    "        cat_cols = make_column_selector(dtype_include=object)\n",
    "        categorical_imputer = Pipeline([\n",
    "            ('Imputer', self.cat_imputer),\n",
    "            ('Encoder', self.encoder)  # Adding encoding for categorical data\n",
    "        ])\n",
    "        imput = ColumnTransformer([\n",
    "            ('categorical_imputer', categorical_imputer, cat_cols),\n",
    "            \n",
    "        ],\n",
    "            remainder='passthrough' # remainder='passthrough' to keep columns not specified\n",
    "        )  \n",
    "    \n",
    "        pipe = Pipeline([\n",
    "            \n",
    "            ('Imputer', imput),\n",
    "            ('DropDuplicateFeatures', DropDuplicateFeatures()),\n",
    "            ('DropConstantFeatures', DropConstantFeatures()),\n",
    "            ('DropCorrelatedFeatures', DropCorrelatedFeatures(threshold=0.95)),\n",
    "            ('scaler', self.scaler),  # Applies scaling to numerical features only\n",
    "        ])\n",
    "\n",
    "        pipe.set_output(transform=\"pandas\")\n",
    "        return pipe\n",
    "\n",
    "# iter_imp=IterativeImputer()\n",
    "\n",
    "# X, y = prep.forward()\n",
    "\n",
    "# X_temp = iter_imp.fit_transform(X,y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d6ae24",
   "metadata": {},
   "source": [
    "# mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "788e834f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.795245Z",
     "start_time": "2024-01-12T19:02:53.791647Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "class Mapper:\n",
    "    # Pipeline for transforming data\n",
    "    # input X\n",
    "    # out transformed X\n",
    "    def __init__(self):\n",
    "        # imputers\n",
    "        self.iter_imputer = True\n",
    "        self.num_imputer = IterativeImputer()\n",
    "        self.cat_imputer =  SimpleImputer(strategy='most_frequent')\n",
    "        # Scale and encoding\n",
    "        self.scaler = RobustScaler(quantile_range=(10.0, 90.0))\n",
    "        self.encoder =  CatBoostEncoder(random_state = 42, drop_invariant=True)\n",
    "    def numerical_imputer(self, num_imputer):\n",
    "        return Pipeline([('Imputer', num_imputer)])\n",
    "    \n",
    "    def categorical_imputer(self, cat_imputer):\n",
    "        return Pipeline([('Imputer', cat_imputer)])\n",
    "    \n",
    "\n",
    "    \n",
    "    def get_type(self, df):\n",
    "        if self.iter_imputer is True:\n",
    "            num_cols = make_column_selector()\n",
    "        else:\n",
    "            num_cols = make_column_selector(dtype_include=np.number)\n",
    "            \n",
    "        cat_cols = make_column_selector(dtype_include=object)\n",
    "\n",
    "        return num_cols, cat_cols\n",
    "    \n",
    "    def pipeline(self, num_cols, cat_cols):\n",
    "        \n",
    "        numerical_imputer = self.numerical_imputer(self.num_imputer)\n",
    "        categorical_imputer = Pipeline([\n",
    "            ('Imputer', self.categorical_imputer(self.cat_imputer)),\n",
    "            ('Encoder', self.encoder)  # Adding encoding for categorical data\n",
    "        ])\n",
    "        imput = ColumnTransformer([\n",
    "            ('numerical_imputer', numerical_imputer, num_cols),\n",
    "            ('categorical_imputer', categorical_imputer, cat_cols),\n",
    "            \n",
    "        ],\n",
    "#             remainder='passthrough' # remainder='passthrough' to keep columns not specified\n",
    "        )  \n",
    "    \n",
    "        pipe = Pipeline([\n",
    "            ('Imputer', imput),\n",
    "            ('DropDuplicateFeatures', DropDuplicateFeatures()),\n",
    "            ('DropConstantFeatures', DropConstantFeatures()),\n",
    "            ('DropCorrelatedFeatures', DropCorrelatedFeatures(threshold=0.95)),\n",
    "            ('scaler', self.scaler),  # Applies scaling to numerical features only\n",
    "        ])\n",
    "\n",
    "        return pipe \n",
    "    def __call__(self, df):\n",
    "        num_cols, cat_cols = self.get_type(df)\n",
    "        pipe = self.pipeline(num_cols, cat_cols)\n",
    "        pipe.set_output(transform=\"pandas\")\n",
    "        return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d53b16b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.799362Z",
     "start_time": "2024-01-12T19:02:53.796065Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "\n",
    "def detect_outliers(df: pd.DataFrame) -> List[int]:\n",
    "    \"\"\"\n",
    "    Detects outliers in a DataFrame using the Interquartile Range (IQR) method.\n",
    "\n",
    "    Outliers are determined based on the IQR, calculated as the difference\n",
    "    between the 85th and 15th percentile of the data. An outlier is defined as\n",
    "    a data point that lies outside the 1.5 * IQR range from the quartiles.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The DataFrame containing numerical data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    multiple_outliers : List[int]\n",
    "        List of index positions in the DataFrame where outliers occur more than 3 times.\n",
    "    \"\"\"\n",
    "    # List to hold the indices of outliers\n",
    "    outlier_indices = []\n",
    "\n",
    "    # Identifying numerical features in the DataFrame\n",
    "    numerical_features = df.select_dtypes(include=['int16', 'float16', 'int32', 'float32', 'int64', 'float64']).columns\n",
    "\n",
    "    for c in numerical_features:\n",
    "        # 1st quartile (15th percentile)\n",
    "        Q1 = np.percentile(df[c], 15)\n",
    "        # 3rd quartile (85th percentile)\n",
    "        Q3 = np.percentile(df[c], 85)\n",
    "        # Interquartile Range (IQR)\n",
    "        IQR = Q3 - Q1\n",
    "        # Outlier step\n",
    "        outlier_step = IQR * 1.5\n",
    "\n",
    "        # Detecting outliers and their indices\n",
    "        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n",
    "\n",
    "        # Storing indices of outliers\n",
    "        outlier_indices.extend(outlier_list_col)\n",
    "\n",
    "    # Counting occurrences of each index\n",
    "    outlier_indices = Counter(outlier_indices)\n",
    "    # Identifying indices with more than 3 occurrences as multiple outliers\n",
    "    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 3)\n",
    "\n",
    "    return multiple_outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b32ea7b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.803079Z",
     "start_time": "2024-01-12T19:02:53.800056Z"
    }
   },
   "outputs": [],
   "source": [
    "def outliers(X, y):\n",
    "    \n",
    "    Outliers_to_drop = detect_outliers(X)\n",
    "    \n",
    "    X = X.drop(Outliers_to_drop, axis=0)\n",
    "    y = y.drop(Outliers_to_drop, axis=0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d32ddb86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.807382Z",
     "start_time": "2024-01-12T19:02:53.803816Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split\n",
    "def generate_comprehensive_features(df):\n",
    "    \"\"\"\n",
    "    Generate comprehensive statistical features for all numerical columns in a DataFrame \n",
    "    and return the DataFrame with the new features included.\n",
    "\n",
    "    :param df: The DataFrame with the original data.\n",
    "    :return: The DataFrame with new statistical feature columns for each numerical column.\n",
    "    \"\"\"\n",
    "    new_features = pd.DataFrame(index=df.index)\n",
    "    numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "#         new_features[f\"{col}_global_mean\"] = [df[col].mean()]* len(df)\n",
    "#         new_features[f\"{col}_global_std\"] = [df[col].std()]* len(df)\n",
    "#         new_features[f\"{col}_global_median\"] = [df[col].median()]* len(df)\n",
    "#         new_features[f\"{col}_global_diff\"] = [df[col].diff()]* len(df)\n",
    "        # Basic statistics\n",
    "        new_features[f'{col}_min'] = [df[col].min()] * len(df)\n",
    "        new_features[f'{col}_max'] = [df[col].max()] * len(df)\n",
    "        new_features[f'{col}_range'] = [df[col].max() - df[col].min()] * len(df)\n",
    "        new_features[f'{col}_sum'] = [df[col].sum()] * len(df)\n",
    "        new_features[f'{col}_var'] = [df[col].var()] * len(df)\n",
    "        new_features[f'{col}_coef_var'] = [df[col].std() / df[col].mean() if df[col].mean() != 0 else 0] * len(df)\n",
    "        new_features[f'{col}_skew'] = [df[col].skew()] * len(df)\n",
    "        new_features[f'{col}_kurt'] = [df[col].kurt()] * len(df)\n",
    "\n",
    "        # Quantiles\n",
    "        new_features[f'{col}_25%'] = [df[col].quantile(0.25)] * len(df)\n",
    "        new_features[f'{col}_50%'] = [df[col].quantile(0.5)] * len(df)\n",
    "        new_features[f'{col}_75%'] = [df[col].quantile(0.75)] * len(df)\n",
    "\n",
    "        # Cumulative statistics\n",
    "        new_features[f'{col}_cumsum'] = df[col].cumsum()\n",
    "        new_features[f'{col}_cummax'] = df[col].cummax()\n",
    "        new_features[f'{col}_cummin'] = df[col].cummin()\n",
    "\n",
    "        # Exponential Moving Average\n",
    "        new_features[f'{col}_ema'] = df[col].ewm(span=10, adjust=False).mean()\n",
    "\n",
    "    # Concatenate the new features with the original DataFrame\n",
    "    df = pd.concat([df, new_features], axis=1)\n",
    "    \n",
    "    return df        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0742ed9b",
   "metadata": {},
   "source": [
    "# Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d5a36084",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.812032Z",
     "start_time": "2024-01-12T19:02:53.808285Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from pandas import DataFrame\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "class TrainTestValidSplit:\n",
    "    \"\"\"\n",
    "    This class provides methods for splitting datasets into training, testing,\n",
    "    and validation sets for machine learning models.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    train_test_split(X, y, test_size, seed):\n",
    "        Splits the dataset into training and validation sets.\n",
    "\n",
    "    cross_valid(X, y, n_splits=5):\n",
    "        Performs cross-validation and returns the split datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    def train_test_split(self, X: DataFrame, y: DataFrame, test_size: float, seed: int) -> Tuple[DataFrame, DataFrame, DataFrame, DataFrame]:\n",
    "        \"\"\"\n",
    "        Splits the dataset into training and validation sets based on the specified test size and random seed.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : DataFrame\n",
    "            Feature dataset.\n",
    "        y : DataFrame\n",
    "            Target dataset.\n",
    "        test_size : float\n",
    "            Proportion of the dataset to include in the test split.\n",
    "        seed : int\n",
    "            Random seed for reproducibility.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_train, X_val, y_train, y_val : Tuple[DataFrame, DataFrame, DataFrame, DataFrame]\n",
    "            Training and validation sets of features and targets.\n",
    "        \"\"\"\n",
    "        # Splitting the dataset into training and validation sets\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size,\n",
    "                                                          random_state=seed,\n",
    "                                                          shuffle=True)\n",
    "        return X_train, X_val, y_train, y_val\n",
    "\n",
    "    def cross_valid(self, X: DataFrame, y: DataFrame, n_splits: int) -> List[Tuple[DataFrame, DataFrame, DataFrame, DataFrame]]:\n",
    "        \"\"\"\n",
    "        Performs K-Fold cross-validation on the dataset.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : DataFrame\n",
    "            Feature dataset.\n",
    "        y : DataFrame\n",
    "            Target dataset.\n",
    "        n_splits : int, optional\n",
    "            Number of folds. Default is 5.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        cross_valid : List[Tuple[DataFrame, DataFrame, DataFrame, DataFrame]]\n",
    "            List of tuples containing the train-test split for each fold.\n",
    "        \"\"\"\n",
    "        kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "        cross_valid = []\n",
    "        for train_index, test_index in kf.split(X):\n",
    "            # Splitting the dataset for each fold\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            cross_valid.append((X_train, X_test, y_train, y_test))\n",
    "        \n",
    "        return cross_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "48ffa175",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.818671Z",
     "start_time": "2024-01-12T19:02:53.813105Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List\n",
    "from sklearn.impute import SimpleImputer, KNNImputer \n",
    "from sklearn.preprocessing import RobustScaler, OrdinalEncoder\n",
    "\n",
    "class Preprocessing:\n",
    "    \"\"\"\n",
    "    Class for preprocessing data for machine learning.\n",
    "\n",
    "    Methods:\n",
    "        read_data: Reads data from a CSV file.\n",
    "        set_type: Converts all columns in the dataframe to numeric types.\n",
    "        select_columns: Filters the dataframe to include only selected columns.\n",
    "        forward: Runs the full preprocessing pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, CFG=None):\n",
    "        \"\"\"\n",
    "        Initializes the Preprocessing class with configuration.\n",
    "\n",
    "        Args:\n",
    "            CFG: Configuration object containing settings like file path and selected columns.\n",
    "        \"\"\"\n",
    "        self.path: str = CFG.PATH\n",
    "        self.selected_columns = CFG.SELECTED_COLUMNS\n",
    "        self.target = CFG.TARGET\n",
    "        self.test_size = CFG.TEST_SIZE\n",
    "        self.seed = CFG.SEED\n",
    "        self.cross_valid = CFG.CROSS_VALID\n",
    "        if self.cross_valid is True:\n",
    "            self.n_splits = CFG.N_SPLIT\n",
    "\n",
    "        \n",
    "        # after optimize set_type remove\n",
    "        self.int_columns = CFG.int_columns\n",
    "        self.float_columns = CFG.float_columns\n",
    "        \n",
    "        self.split = TrainTestValidSplit()\n",
    "        \n",
    "    def read_data(self, path: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Reads data from the given CSV file path.\n",
    "\n",
    "        Args:\n",
    "            path: The file path of the CSV data.\n",
    "\n",
    "        Returns:\n",
    "            DataFrame containing the read data.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return pd.read_csv(path, low_memory=False, dtype='str')\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"CSV file not found at the specified path: {path}\")\n",
    "        except IOError as e:\n",
    "            raise IOError(f\"Error occurred while reading the CSV file at {path}: {e}\")\n",
    "    \n",
    "    def clinning(self, df):\n",
    "        for column in df.columns:\n",
    "            df[column] = df[column].astype(str).str.strip().str.replace(' ', '').copy()\n",
    "            df[column] = df[column].astype(str).str.strip().str.replace(',', '.').copy()\n",
    "        return df\n",
    "    \n",
    "    def find_type(self, df):\n",
    "        for column in df.columns:\n",
    "            df[column] = df[column].astype(str).str.strip().replace(' ', '').replace(',', '.')\n",
    "        for column in self.int_columns:\n",
    "            df[column] = df[column].astype(int)\n",
    "        for column in self.float_columns:\n",
    "            df[column] = df[column].astype(float)\n",
    "        return df\n",
    "    \n",
    "    def select_columns(self, df: pd.DataFrame, selected_columns: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Filters the dataframe to include only selected columns.\n",
    "\n",
    "        Args:\n",
    "            df: The dataframe to be filtered.\n",
    "            selected_columns: The list of column names to be included.\n",
    "\n",
    "        Returns:\n",
    "            Filtered DataFrame.\n",
    "        \"\"\"\n",
    "        return df[selected_columns]\n",
    "    \n",
    "    \n",
    "    def handle_mixed_types(self, df):\n",
    "        for column in df.columns:\n",
    "            if df[column].apply(type).nunique() > 1:\n",
    "                df[column + '_num'] = pd.to_numeric(df[column], errors='coerce')\n",
    "                df[column + '_cat'] = df[column].where(df[column + '_num'].isna())\n",
    "        return df\n",
    "    \n",
    "    def get_X_y(self, df, target):\n",
    "        \n",
    "        df = df.drop('Кадастровый_номер_ob', axis=1).copy()\n",
    "        X = df.drop(target, axis=1).copy()\n",
    "        y = df[target].copy()\n",
    "        return X, y\n",
    "    \n",
    "    def etl(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Runs the full preprocessing pipeline.\n",
    "\n",
    "        Returns:\n",
    "            Preprocessed DataFrame ready for machine learning tasks.\n",
    "        \"\"\"\n",
    "        df = self.read_data(self.path)\n",
    "        df = self.select_columns(df, self.selected_columns)\n",
    "        df = df.drop_duplicates(subset = ['Кадастровый_номер_ob', 'дата_ob', self.target])\n",
    "        df = self.clinning(df)\n",
    "        df = self.find_type(df)\n",
    "        df = self.handle_mixed_types(df.copy())\n",
    "        \n",
    "        return df\n",
    "    def forward(self) -> pd.DataFrame:\n",
    "        df = self.etl()\n",
    "        X, y = self.get_X_y(df, self.target)\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e11c8849",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.822445Z",
     "start_time": "2024-01-12T19:02:53.819422Z"
    }
   },
   "outputs": [],
   "source": [
    "# prep = Preprocessing(CFG, Mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ecc9bbd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.825556Z",
     "start_time": "2024-01-12T19:02:53.823065Z"
    }
   },
   "outputs": [],
   "source": [
    "# prep.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "dbec34cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.828647Z",
     "start_time": "2024-01-12T19:02:53.826190Z"
    }
   },
   "outputs": [],
   "source": [
    "prep = Preprocessing(CFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112b7d8a",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0bfb8589",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.831757Z",
     "start_time": "2024-01-12T19:02:53.829330Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor, StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "ddf8ea31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.834941Z",
     "start_time": "2024-01-12T19:02:53.832640Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.isotonic import IsotonicRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "460962dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.838036Z",
     "start_time": "2024-01-12T19:02:53.835601Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklego.linear_model import LADRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "823c422f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.843141Z",
     "start_time": "2024-01-12T19:02:53.839873Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from sklearn.ensemble import ExtraTreesRegressor, GradientBoostingRegressor, StackingRegressor, HistGradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "class Model:\n",
    "    def __init__(self, CFG: Any) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the Model with configuration parameters.\n",
    "\n",
    "        Parameters:\n",
    "        CFG (Any): Configuration object containing settings like random seed and number of jobs.\n",
    "\n",
    "        Attributes:\n",
    "        random_state (int): Random state seed for reproducibility.\n",
    "        n_jobs (int): Number of parallel jobs to run.\n",
    "        tree (ExtraTreesRegressor): Extra Trees regressor model.\n",
    "        grad (GradientBoostingRegressor): Gradient Boosting regressor model.\n",
    "        xgb (XGBRegressor): XGBoost regressor model.\n",
    "        lgbm (LGBMRegressor): LightGBM regressor model.\n",
    "        hist (HistGradientBoostingRegressor): Histogram-based Gradient Boosting Regression Tree model.\n",
    "        model (NoneType): Placeholder for the final model, to be defined later.\n",
    "        pipeline (Pipeline): The pipeline that will contain preprocessing and the stacking regressor.\n",
    "        \"\"\"\n",
    "        self.random_state: int = CFG.SEED\n",
    "        self.n_jobs: int = CFG.N_JOBS\n",
    "#         self.tree: ExtraTreesRegressor = HistGradientBoostingRegressor(random_state=self.random_state,warm_start=True, max_iter=50)\n",
    "        self.tree: ExtraTreesRegressor = LADRegression()\n",
    "        self.grad: GradientBoostingRegressor = GradientBoostingRegressor(random_state=self.random_state)\n",
    "        self.xgb: XGBRegressor = XGBRegressor(random_state=self.random_state, n_jobs=self.n_jobs)\n",
    "        self.lgbm: LGBMRegressor = LGBMRegressor(\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=self.n_jobs,\n",
    "            verbose=0,\n",
    "            force_row_wise=True,\n",
    "            objective='mae'\n",
    "        )\n",
    "        if CFG.BAGGING is True:\n",
    "            self.hist: HistGradientBoostingRegressor = BaggingRegressor(estimator=HistGradientBoostingRegressor(random_state=self.random_state, warm_start=True, max_iter=50),\n",
    "                                                                   random_state=CFG.SEED, n_jobs=CFG.N_JOBS, n_estimators=CFG.BAGGING_ESTIMATORS)\n",
    "        else:\n",
    "            self.hist: HistGradientBoostingRegressor = HistGradientBoostingRegressor(random_state=self.random_state, warm_start=True, max_iter=50)\n",
    "        self.model = None\n",
    "        self.initialize_model()\n",
    "\n",
    "    def initialize_model(self) -> Pipeline:\n",
    "        \"\"\"\n",
    "        Initialize the stacking regressor model.\n",
    "\n",
    "        Returns:\n",
    "        Pipeline: A pipeline object that includes the stacking regressor.\n",
    "        \"\"\"\n",
    "        base_estimators = [\n",
    "            ('ExtraTree', self.tree),  # Uncomment if you want to include ExtraTreesRegressor in your stacking\n",
    "            ('GradientBoost', self.grad),\n",
    "            ('XGBoost', self.xgb),\n",
    "            ('LightGBM', self.lgbm),\n",
    "        ]\n",
    "        \n",
    "        stack_reg: StackingRegressor = StackingRegressor(estimators=base_estimators, final_estimator=self.hist, n_jobs=self.n_jobs)\n",
    "        self.pipeline: Pipeline = Pipeline([\n",
    "            ('stacking_regressor', stack_reg)\n",
    "        ])\n",
    "\n",
    "        return self.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "4637cffa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.846773Z",
     "start_time": "2024-01-12T19:02:53.843790Z"
    }
   },
   "outputs": [],
   "source": [
    "Preprocessing(CFG)\n",
    "split = TrainTestValidSplit()\n",
    "model = Model(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "599a19cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.860133Z",
     "start_time": "2024-01-12T19:02:53.847389Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ColumnRenamer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, prefixes_to_remove):\n",
    "        self.prefixes_to_remove = prefixes_to_remove\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self  # Nothing to do here\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for prefix in self.prefixes_to_remove:\n",
    "            X.columns = [col.replace(prefix, '') for col in X.columns]\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdbde4a",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8ce84d02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.867884Z",
     "start_time": "2024-01-12T19:02:53.860855Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, r2_score, median_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from utils.TrainTestValidSplit import TrainTestValidSplit\n",
    "from preprocessing.Preprocessing import Preprocessing\n",
    "from utils.detect_outliers import outliers\n",
    "from utils.ColumnRenamer import ColumnRenamer\n",
    "\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "class Run:\n",
    "    \"\"\"\n",
    "    A class for managing machine learning pipelines, including preprocessing, training,\n",
    "    cross-validation, evaluation, and model serialization.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    CFG : Any\n",
    "        Configuration object containing settings for the run.\n",
    "    mapper : Any\n",
    "        Object responsible for mapping data transformations.\n",
    "    model : Any\n",
    "        Machine learning model to be used.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    initialize_pipeline(X_train):\n",
    "        Initializes the processing pipeline with training data.\n",
    "\n",
    "    make_weight_tune():\n",
    "        Performs weight tuning on the model.\n",
    "\n",
    "    run():\n",
    "        Executes the pipeline, including data splitting, preprocessing, training, and evaluation.\n",
    "\n",
    "    metrics():\n",
    "        Computes and prints various evaluation metrics.\n",
    "\n",
    "    get_model():\n",
    "        Returns the trained model pipeline.\n",
    "\n",
    "    save_model():\n",
    "        Saves the model to a file.\n",
    "\n",
    "    load_model(path=None):\n",
    "        Loads a model from a file.\n",
    "    \"\"\"\n",
    "    def __init__(self, CFG, mapper, model) -> None:\n",
    "        self.save_path = CFG.SAVE_PATH\n",
    "        self.load_path = CFG.LOAD_PATH\n",
    "        self.save_method = CFG.SAVE_METHOD\n",
    "        self.test_size = CFG.TEST_SIZE\n",
    "        self.seed = CFG.SEED\n",
    "        self.target = CFG.TARGET\n",
    "        self.cross_valid = CFG.CROSS_VALID\n",
    "\n",
    "        if self.cross_valid is True:\n",
    "            self.n_splits = CFG.N_SPLIT\n",
    "        self.mapper = mapper\n",
    "        self.split = TrainTestValidSplit()\n",
    "        self.model = model\n",
    "\n",
    "        self.prep = Preprocessing(CFG)\n",
    "        self.column_renamer = ColumnRenamer(prefixes_to_remove=[\n",
    "            'numerical_imputer__', 'categorical_imputer__'\n",
    "        ])\n",
    "        self.pipe = None\n",
    "        self.df1 = pd.DataFrame()\n",
    "\n",
    "    def initialize_pipeline(self, X_train) -> None:\n",
    "        mapper = self.mapper(X_train)\n",
    "\n",
    "        self.pipe = Pipeline([\n",
    "            ('mapper', mapper),\n",
    "            ('Column_renamer', self.column_renamer),\n",
    "#             ('CustomTransformer',CustomTransformer()),\n",
    "#             ('GenerateFeatures', GenerateFeatures()),\n",
    "            ('model', self.model.pipeline)])\n",
    "    def make_weight_tune(self, X_train, y_train, base_weight=0.01, lower=0.001, upper=0.06):\n",
    "        \"\"\"\n",
    "        Performs weight tuning on the model.\n",
    "        \"\"\"\n",
    "        self.pipe.fit(X_train, y_train)\n",
    "        y_train_pred = self.pipe.predict(X_train)\n",
    "        y_train_error = np.abs(y_train - y_train_pred)\n",
    "        sample_weights = base_weight + ((lower < y_train_error) & (y_train_error < upper)).astype(float)\n",
    "\n",
    "        # Correctly pass sample_weight to the specific model step\n",
    "        fit_params = {'model__stacking_regressor__sample_weight': sample_weights}\n",
    "        self.pipe.fit(X_train, y_train, **fit_params)\n",
    "\n",
    "\n",
    "\n",
    "    def run(self) -> Optional[Pipeline]:\n",
    "#         split (think about it maybe delete from here)\n",
    "#         transform\n",
    "#         feature selection and etc\n",
    "#         model\n",
    "        if self.cross_valid is False:\n",
    "            X, y = self.prep.forward()\n",
    "\n",
    "            X_train, X_val, y_train, y_val = self.split.train_test_split(\n",
    "                X, y, self.test_size, self.seed)\n",
    "            self.X_val, self.y_val = X_val, y_val\n",
    "            X_train, y_train = outliers(X_train, y_train)\n",
    "            # initialize pipeline\n",
    "            self.initialize_pipeline(X_train)\n",
    "\n",
    "            self.pipe.fit(X_train, y_train)\n",
    "\n",
    "            return self.pipe\n",
    "        else:\n",
    "            X, y = self.prep.forward()\n",
    "            X_train, X_val, y_train, y_val = self.split.train_test_split(\n",
    "                    X, y, self.test_size, self.seed)\n",
    "\n",
    "            self.X_val, self.y_val = X_val, y_val\n",
    "            X_train, y_train = outliers(X_train, y_train)\n",
    "            cross_valid_splits = self.split.cross_valid(\n",
    "                X_train, y_train, self.n_splits)\n",
    "            n_splits = len(cross_valid_splits)  # Number of splits\n",
    "            # initialize pipeline\n",
    "            self.initialize_pipeline(X_train)\n",
    "\n",
    "#             score_without, score_with = 0, 0\n",
    "            for X_train, X_test, y_train, y_test in cross_valid_splits:\n",
    "                \n",
    "#                 self.pipe.fit(X_train, y_train)\n",
    "                # Performs weight tuning on the model.\n",
    "                self.make_weight_tune(X_train, y_train)\n",
    "\n",
    "    def metrics(self, model) -> None:\n",
    "        y_pred = model.predict(self.X_val)\n",
    "        print('MAE', mean_absolute_error(self.y_val, y_pred))\n",
    "        print('MAPE', mean_absolute_percentage_error(self.y_val, y_pred))\n",
    "        print('R2_score', r2_score(self.y_val, y_pred))\n",
    "        print('mean', y_pred.mean())\n",
    "        print('std', y_pred.std())\n",
    "\n",
    "    def get_model(self) -> Pipeline:\n",
    "        return self.pipe\n",
    "\n",
    "    def save_model(self) -> None:\n",
    "        if self.save_method == 'pickle':\n",
    "            import pickle\n",
    "            pickle.dump(self.pipe, open(f\"saved_models/{self.save_path}.pickle\", 'wb'))\n",
    "\n",
    "    def load_model(self, path = None):\n",
    "        if self.save_method == 'pickle':\n",
    "            import pickle\n",
    "            self.pipe = pickle.load(open(f\"saved_models/{self.load_path}.pickle\", 'rb'))\n",
    "            print('model loaded')\n",
    "            self.initialize_predict()\n",
    "\n",
    "    def initialize_predict(self):\n",
    "        df1 = pd.DataFrame()\n",
    "        df = self.prep.etl()\n",
    "        df1  = df[['Кадастровый_номер_ob', self.target]].copy()\n",
    "        X, y = self.prep.get_X_y(df, self.target)\n",
    "        df1[self.target] = self.pipe.predict(X)\n",
    "        self.df1 = df1\n",
    "\n",
    "    def for_api(self, kadastr,df=None):\n",
    "        res_df = self.df1[self.df1['Кадастровый_номер_ob'] == kadastr]\n",
    "        res = res_df.to_json(orient=\"records\")\n",
    "        parsed = json.loads(res)\n",
    "        return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "679b7df5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.871542Z",
     "start_time": "2024-01-12T19:02:53.868766Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d5e468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "10da2260",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.874658Z",
     "start_time": "2024-01-12T19:02:53.872850Z"
    }
   },
   "outputs": [],
   "source": [
    "# import skops.io as sio\n",
    "# obj = sio.dump(model, \"saved_models/model.skops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "fd5c6727",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T19:02:53.877639Z",
     "start_time": "2024-01-12T19:02:53.875400Z"
    }
   },
   "outputs": [],
   "source": [
    "# k = pickle.load(open(f\"saved_models/{CFG.LOAD_PATH}.pickle\", 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65160c9d",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b18fc9f",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-12T19:02:53.739Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterations: 100%|██████████| 3/3 [00:07<00:00,  2.34s/it]\n",
      "Iterations: 100%|██████████| 3/3 [00:07<00:00,  2.45s/it]\n",
      "Iterations:  33%|███▎      | 1/3 [00:02<00:04,  2.05s/it]"
     ]
    }
   ],
   "source": [
    "pipe = Run(CFG, Mapper(), Model(CFG))\n",
    "\n",
    "pipe.run()\n",
    "model = pipe.get_model()\n",
    "pipe.metrics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e453bc2b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-12T19:02:53.740Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe.metrics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59afe7fa",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-12T19:02:53.741Z"
    }
   },
   "outputs": [],
   "source": [
    "# MAE 55974.17974005252\n",
    "# MAPE 0.10650260600282552\n",
    "# R2_score 0.5877333226486814\n",
    "# mean 543468.5212345918\n",
    "# std 92553.38307449795"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80e999b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-12T19:02:53.741Z"
    }
   },
   "outputs": [],
   "source": [
    "# MAE 57455.71822476894\n",
    "# MAPE 0.10879462453450428\n",
    "# R2_score 0.5540224099434927\n",
    "# mean 542924.7683698069\n",
    "# std 90117.77702643277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27375cf",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-12T19:02:53.742Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "# MAE 58559.68338508163\n",
    "# MAPE 0.11048791721865418\n",
    "# R2_score 0.5394781775327182\n",
    "# mean 543792.2710455992\n",
    "# std 89703.40279469808"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a91e6d",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-12T19:02:53.742Z"
    }
   },
   "outputs": [],
   "source": [
    "# df = pipe.for_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9755049",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-12T19:02:53.743Z"
    }
   },
   "outputs": [],
   "source": [
    "# df['Кадастровый_номер_ob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af68a75",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-12T19:02:53.743Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780cd15c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-12T19:02:53.744Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9971633",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-12T19:02:53.744Z"
    }
   },
   "outputs": [],
   "source": [
    "def for_api(self, df=None):\n",
    "    \n",
    "    df1 = pd.DataFrame()\n",
    "    df = self.prep.etl()\n",
    "    df1  = df[['Кадастровый_номер_ob', self.target]].copy()\n",
    "    df = self.prep.etl()\n",
    "    X, y = self.prep.get_X_y(df, self.target)\n",
    "    df1[self.target] = self.pipe.predict(X)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94700c47",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-12T19:02:53.745Z"
    }
   },
   "outputs": [],
   "source": [
    "model = pipe.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b85f31",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-12T19:02:53.745Z"
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dfc9c4",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-12T19:02:53.746Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "# # let's save the model\n",
    "# model_path = \"example.pkl\"\n",
    "# local_repo = \"my-awesome-model\"\n",
    "# with open(model_path, mode=\"bw\") as f:\n",
    "#     pickle.dump(pipe, file=f)\n",
    "# # we will now initialize a local repository\n",
    "# hub_utils.init(\n",
    "#     model=model_path,\n",
    "#     requirements=[f\"scikit-learn={sklearn.__version__}\"],\n",
    "#     dst=local_repo,\n",
    "#     task=\"tabular-classification\",\n",
    "#     data=X_test,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78db8497",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-12T19:02:53.746Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = prep.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30ac493",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-12T19:02:53.747Z"
    }
   },
   "outputs": [],
   "source": [
    "# X = X.drop_duplicates(subset=['дата_ob', ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deb43fe",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-12T19:02:53.747Z"
    }
   },
   "outputs": [],
   "source": [
    "def metrics(X,y,model) -> None:\n",
    "        y_pred = model.predict(X)\n",
    "        print('MAE', mean_absolute_error(y, y_pred))\n",
    "        print('MAPE', mean_absolute_percentage_error(y, y_pred))\n",
    "        print('R2_score', r2_score(y, y_pred))\n",
    "        print('mean', y_pred.mean())\n",
    "        print('std', y_pred.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b47e73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-12T15:09:12.162904Z",
     "start_time": "2024-01-12T15:09:12.160598Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adaade0",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-12T19:02:53.748Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics(X,y,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fe772c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-01-12T19:02:53.749Z"
    }
   },
   "outputs": [],
   "source": [
    "# MAE 47959.73187672039\n",
    "# MAPE 0.09088530212124311\n",
    "# R2_score 0.6566025139314876\n",
    "# mean 541942.2512662262\n",
    "# std 90440.25246178386"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca531b48",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0e267c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7101fe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a738002a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
